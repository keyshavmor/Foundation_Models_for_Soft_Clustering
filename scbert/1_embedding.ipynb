{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e48b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from performer_pytorch import PerformerLM\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from utils import *\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972f9a1b-7d2a-4bfd-881c-898824906c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS = 7\n",
    "SEED = 2021\n",
    "EPOCHS = 1\n",
    "SEQ_LEN = 16906 + 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "007090a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/test.h5ad'\n",
    "adata = sc.read_h5ad(data_path)\n",
    "data = adata.layers['log1p_norm']\n",
    "print(type(data))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b158503-542d-4dec-9a6a-32b9b86579a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_columns = (adata.X.toarray() != 0).any(axis=0)\n",
    "non_zero_columns = np.append(non_zero_columns, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2951c01-93a7-425d-8310-0622ae585082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without gene2vec\n",
    "model = PerformerLM(\n",
    "    num_tokens = CLASS,\n",
    "    dim = 200,\n",
    "    depth = 6,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    heads = 10,\n",
    "    local_attn_heads = 0,\n",
    "    g2v_position_emb = False\n",
    ")\n",
    "\n",
    "path = 'model/scbert_pretrain.pth'\n",
    "ckpt = torch.load(path, map_location='cpu')\n",
    "model_state_dict = ckpt['model_state_dict']\n",
    "# Filter out unexpected keys\n",
    "model_state_dict = {k: v for k, v in model_state_dict.items() \n",
    "                    if k in model.state_dict()}\n",
    "# Now load the filtered state dict\n",
    "model.load_state_dict(model_state_dict, strict=False)  # strict=False ignores missing keys\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78c9e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scbert_embed(data):\n",
    "    batch_size = data.shape[0]\n",
    "    model.eval()\n",
    "    batch = []\n",
    "    epoch = []\n",
    "    with torch.no_grad():\n",
    "        for index in tqdm(range(batch_size)):\n",
    "            full_seq = data[index].toarray()[0]\n",
    "            full_seq[full_seq > (CLASS - 2)] = CLASS - 2\n",
    "            full_seq = torch.from_numpy(full_seq).long()\n",
    "            full_seq = torch.cat((full_seq, torch.tensor([0]))).to(device)\n",
    "            full_seq = full_seq.unsqueeze(0)\n",
    "            cell_embedding = model(full_seq, return_encodings = True, output_attentions = False)\n",
    "            cell_embedding = torch.reshape(cell_embedding, [2001, 200])\n",
    "            #cell_embedding = cell_embedding[non_zero_columns, :]\n",
    "            batch.append(cell_embedding)\n",
    "            if index % 500 == 0:\n",
    "                #regularly empty GPU of data\n",
    "                epoch.extend([b.cpu() for b in batch])\n",
    "                batch = []\n",
    "        epoch.extend([b.cpu() for b in batch])\n",
    "    embeddings = np.stack([t.numpy().astype(np.float16) for t in epoch])\n",
    "    return embeddings\n",
    "\n",
    "def get_cuts(N, k):\n",
    "    return [i * N // k for i in range(1, k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25fc1657-095e-4724-9dc1-f0eb3427c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batch = 1\n",
    "batch_cut = get_cuts(2000, number_of_batch)\n",
    "batch_cut = [2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbd8a312-6e54-4756-8f50-e69074ec8c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "batch : 0-2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21856e8284654a8198b2a490650caebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2001, 200)\n"
     ]
    }
   ],
   "source": [
    "for i in range(number_of_batch):\n",
    "    print(i)\n",
    "    if i == 0:\n",
    "        batch_data = data[:batch_cut[i]]\n",
    "        print(f'batch : {0}-{batch_cut[i]}')\n",
    "    elif i == len(batch_cut):\n",
    "        batch_data = data[batch_cut[i-1]:]\n",
    "        print(f'batch : {batch_cut[i-1]}-{data.shape[0]}')\n",
    "    else:\n",
    "        batch_data = data[batch_cut[i-1]:batch_cut[i]]\n",
    "        print(f'batch : {batch_cut[i-1]}-{batch_cut[i]}')\n",
    "    embeddings = scbert_embed(batch_data)\n",
    "    print(embeddings.shape)\n",
    "    path = 'data/embeddings_' + str(i) + '.npy'\n",
    "    np.save(path, embeddings)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
